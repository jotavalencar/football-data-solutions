# -*- coding: utf-8 -*-
"""MetricaCampeoes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMwNVMqHctCNsDwgjpchg--vDmzsFCe4

# Método de Cotovelo
"""

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np

def process_excel(file_path):
    # Lendo o arquivo Excel
    xls = pd.ExcelFile(file_path)

    for sheet_name in xls.sheet_names:
        print(f"Processando a aba: {sheet_name}")

        # Lendo a aba específica
        data = xls.parse(sheet_name)

        # Selecionando as colunas a partir de K
        data = data.iloc[:, 10:]

        # Identificando colunas que contêm 'accuracy'
        accuracy_cols = [col for col in data.columns if 'accuracy' in col]

        # Normalizar as colunas de accuracy para escala [0,1]
        if accuracy_cols:
            min_max_scaler = MinMaxScaler()
            data[accuracy_cols] = min_max_scaler.fit_transform(data[accuracy_cols])

        # Padronizar o restante das colunas
        scaler = StandardScaler()
        data = scaler.fit_transform(data)

        # Método de Cotovelo para determinar o número ideal de clusters
        distortions = []
        K = range(1, 10)
        for k in K:
            kmeanModel = KMeans(n_clusters=k)
            kmeanModel.fit(data)
            distortions.append(kmeanModel.inertia_)

        # Cálculo da distância euclidiana
        x1, y1 = 1, distortions[0]
        x2, y2 = len(K), distortions[-1]

        distances = []
        for i in range(len(distortions)):
            x0 = i+1
            y0 = distortions[i]
            numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)
            denominator = ((y2 - y1)**2 + (x2 - x1)**2)**0.5
            distances.append(numerator/denominator)

        optimal_clusters = distances.index(max(distances)) + 1

        # Plotando o gráfico de cotovelo
        plt.figure(figsize=(10,5))
        plt.plot(K, distortions, 'bx-')
        plt.xlabel('Número de clusters')
        plt.ylabel('Distortion')
        plt.title(f'Método de Cotovelo para a aba {sheet_name}')
        plt.show()

        print(f"Número ótimo de clusters para {sheet_name}: {optimal_clusters}\n")

# Coloque aqui o caminho do seu arquivo
file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'
process_excel(file_path)

"""# K Means - T1"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import pairwise_distances_argmin_min

# Caminho para o arquivo
file_path = "/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx"

# Número ideal de clusters para cada aba
optimal_clusters = {
    "Atacantes": 3,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 3,
    "Laterais": 3,
    "Zagueiros": 3,
    "Goleiros": 3
}

# Função para plotar os clusters
def plot_clusters(data_2d, kmeans_labels, centroids_2d, title):
    plt.figure(figsize=(8, 6))

    # Plotando os pontos dos clusters
    scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=kmeans_labels, cmap='rainbow', edgecolors='k', s=100)

    # Plotando os centróides e associando a legenda
    for i, centroid in enumerate(centroids_2d):
        plt.scatter(centroid[0], centroid[1], c='black', s=200, marker='X', label=f"Centroid {i+1}")

    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend(handles=scatter.legend_elements()[0], title="Clusters")
    plt.show()

# Processando cada aba do arquivo
xls = pd.ExcelFile(file_path)
for sheet_name in optimal_clusters:
    data = pd.read_excel(xls, sheet_name=sheet_name)

    # Selecionando as colunas a partir de K
    data_selected = data.iloc[:, 10:]

    # Padronizando os dados
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_selected)

    # Executando o KMeans com o número ótimo de clusters
    kmeans = KMeans(n_clusters=optimal_clusters[sheet_name], random_state=0)
    kmeans.fit(data_scaled)

    # Transformando os dados em 2D para a plotagem
    pca = PCA(n_components=2)
    data_2d = pca.fit_transform(data_scaled)

    # Obter os centróides e transformá-los em 2D
    centroids_2d = pca.transform(kmeans.cluster_centers_)

    # Plotando os clusters
    plot_clusters(data_2d, kmeans.labels_, centroids_2d, title=sheet_name)

    # Identificando os pontos mais próximos dos centróides
    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, data_scaled)
    print(f"Linhas do Excel mais próximas dos centróides para {sheet_name}: {closest + 2}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances_argmin_min

# Caminho para o arquivo
file_path = "/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx"

# Número ideal de clusters para cada aba (alterado conforme sua observação)
optimal_clusters = {
    "Atacantes": 3,
    "Pontas": 2,
    "Meio Ofensivos": 2,  # Modificado conforme sua observação
    "Volantes": 3,
    "Laterais": 3,
    "Zagueiros": 3,
    "Goleiros": 3
}

# Função para plotar os clusters
def plot_clusters(data_2d, kmeans_labels, centroids_2d, title):
    plt.figure(figsize=(8, 6))

    scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=kmeans_labels, cmap='rainbow', edgecolors='k', s=100)
    for i, centroid in enumerate(centroids_2d):
        plt.scatter(centroid[0], centroid[1], c='black', s=200, marker='X', label=f"Centroid {i+1}")

    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend(handles=scatter.legend_elements()[0], title="Clusters")
    plt.show()

# Processando cada aba do arquivo
xls = pd.ExcelFile(file_path)
for sheet_name in optimal_clusters:
    data = pd.read_excel(xls, sheet_name=sheet_name)

    # Selecionando as colunas a partir de K
    data_selected = data.iloc[:, 10:]

    # Aplicando a transformação logarítmica
    data_log_transformed = np.log1p(data_selected)

    # Padronizando os dados após a transformação logarítmica
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_log_transformed)

    # Executando o KMeans com o número ótimo de clusters
    kmeans = KMeans(n_clusters=optimal_clusters[sheet_name], random_state=0)
    kmeans.fit(data_scaled)

    # Transformando os dados em 2D para a plotagem
    pca = PCA(n_components=2)
    data_2d = pca.fit_transform(data_scaled)

    # Obter os centróides e transformá-los em 2D
    centroids_2d = pca.transform(kmeans.cluster_centers_)

    # Plotando os clusters
    plot_clusters(data_2d, kmeans.labels_, centroids_2d, title=sheet_name)

    # Identificando os pontos mais próximos dos centróides
    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, data_scaled)
    print(f"Linhas do Excel mais próximas dos centróides para {sheet_name}: {closest + 2}")

"""# Coeficiente Silhueta - T1"""

import matplotlib.cm as cm
from sklearn.metrics import silhouette_samples, silhouette_score

# Suponha que os dados já foram carregados, transformados e o modelo KMeans já foi treinado.

# Dicionário com o número ótimo de clusters para cada posição (altere conforme necessário)
optimal_clusters = {
    "Atacantes": 3,
    "Pontas": 2,
    "Meio Ofensivos": 2, # Modificado conforme sua observação
    "Volantes": 3,
    "Laterais": 3,
    "Zagueiros": 3,
    "Goleiros": 3
}

for sheet_name, n_clusters in optimal_clusters.items():
    data = pd.read_excel(file_path, sheet_name=sheet_name)
    data = data.iloc[:, 10:]

    # Transformação logarítmica e padronização
    data = np.log1p(data)
    scaled_data = StandardScaler().fit_transform(data)

    # Treinando o KMeans com o número ótimo de clusters
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)

    # Calculando o coeficiente médio de silhueta para todos os pontos
    silhouette_avg = silhouette_score(scaled_data, cluster_labels)
    print(f"Para {sheet_name} com n_clusters = {n_clusters}, o coeficiente médio de silhueta é: {silhouette_avg:.2f}")

    # Calculando os coeficientes de silhueta para cada amostra
    sample_silhouette_values = silhouette_samples(scaled_data, cluster_labels)

    fig, ax = plt.subplots()
    ax.set_xlim([-0.1, 1])
    ax.set_ylim([0, len(scaled_data) + (n_clusters + 1) * 10])

    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax.set_title(f"Análise da Silhueta para {sheet_name} com n_clusters = {n_clusters}")
    ax.set_xlabel("Valor do coeficiente de silhueta")
    ax.set_ylabel("Cluster label")
    ax.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax.set_yticks([])
    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.show()

"""# K Means - T2"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances_argmin_min

# Caminho para o arquivo
file_path = "/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx"

# Número ideal de clusters para cada aba (alterado conforme sua observação)
optimal_clusters = {
    "Atacantes": 2,
    "Pontas": 2,
    "Meio Ofensivos": 2,  # Modificado conforme sua observação
    "Volantes": 2,
    "Laterais": 2,
    "Zagueiros": 2,
    "Goleiros": 3
}

# Função para plotar os clusters
def plot_clusters(data_2d, kmeans_labels, centroids_2d, title):
    plt.figure(figsize=(8, 6))

    scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=kmeans_labels, cmap='rainbow', edgecolors='k', s=100)
    for i, centroid in enumerate(centroids_2d):
        plt.scatter(centroid[0], centroid[1], c='black', s=200, marker='X', label=f"Centroid {i+1}")

    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend(handles=scatter.legend_elements()[0], title="Clusters")
    plt.show()

# Processando cada aba do arquivo
xls = pd.ExcelFile(file_path)
for sheet_name in optimal_clusters:
    data = pd.read_excel(xls, sheet_name=sheet_name)

    # Selecionando as colunas a partir de K
    data_selected = data.iloc[:, 10:]

    # Aplicando a transformação logarítmica
    data_log_transformed = np.log1p(data_selected)

    # Padronizando os dados após a transformação logarítmica
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_log_transformed)

    # Executando o KMeans com o número ótimo de clusters
    kmeans = KMeans(n_clusters=optimal_clusters[sheet_name], random_state=0)
    kmeans.fit(data_scaled)

    # Transformando os dados em 2D para a plotagem
    pca = PCA(n_components=2)
    data_2d = pca.fit_transform(data_scaled)

    # Obter os centróides e transformá-los em 2D
    centroids_2d = pca.transform(kmeans.cluster_centers_)

    # Plotando os clusters
    plot_clusters(data_2d, kmeans.labels_, centroids_2d, title=sheet_name)

    # Identificando os pontos mais próximos dos centróides
    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, data_scaled)
    print(f"Linhas do Excel mais próximas dos centróides para {sheet_name}: {closest + 2}")

"""# Coeficiente Silhueta - T2"""

import matplotlib.cm as cm
from sklearn.metrics import silhouette_samples, silhouette_score

# Suponha que os dados já foram carregados, transformados e o modelo KMeans já foi treinado.

# Dicionário com o número ótimo de clusters para cada posição (altere conforme necessário)
optimal_clusters = {
    "Atacantes": 2,
    "Pontas": 2,
    "Meio Ofensivos": 2,  # Modificado conforme sua observação
    "Volantes": 2,
    "Laterais": 2,
    "Zagueiros": 2,
    "Goleiros": 3
}

for sheet_name, n_clusters in optimal_clusters.items():
    data = pd.read_excel(file_path, sheet_name=sheet_name)
    data = data.iloc[:, 10:]

    # Transformação logarítmica e padronização
    data = np.log1p(data)
    scaled_data = StandardScaler().fit_transform(data)

    # Treinando o KMeans com o número ótimo de clusters
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)

    # Calculando o coeficiente médio de silhueta para todos os pontos
    silhouette_avg = silhouette_score(scaled_data, cluster_labels)
    print(f"Para {sheet_name} com n_clusters = {n_clusters}, o coeficiente médio de silhueta é: {silhouette_avg:.2f}")

    # Calculando os coeficientes de silhueta para cada amostra
    sample_silhouette_values = silhouette_samples(scaled_data, cluster_labels)

    fig, ax = plt.subplots()
    ax.set_xlim([-0.1, 1])
    ax.set_ylim([0, len(scaled_data) + (n_clusters + 1) * 10])

    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax.set_title(f"Análise da Silhueta para {sheet_name} com n_clusters = {n_clusters}")
    ax.set_xlabel("Valor do coeficiente de silhueta")
    ax.set_ylabel("Cluster label")
    ax.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax.set_yticks([])
    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.show()

"""# K Means - T1 3D"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Carregando os dados
sheets = ['Atacantes', 'Pontas', 'Meio Ofensivos', 'Volantes', 'Laterais', 'Zagueiros', 'Goleiros']
file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'

# Dicionário para o número de clusters
cluster_dict = {
    "Atacantes": 3,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 3,
    "Laterais": 3,
    "Zagueiros": 3,
    "Goleiros": 3
}

# Para cada aba, aplicar K-means e visualizar em 3D
for sheet in sheets:
    data = pd.read_excel(file_path, sheet_name=sheet)
    data = data.select_dtypes(include=[np.number])  # Seleciona apenas colunas numéricas

    # Padronização dos dados
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)

    # Aplicando PCA para 3 componentes principais
    pca = PCA(n_components=3)
    principal_components = pca.fit_transform(scaled_data)

    # Aplicando K-means
    kmeans = KMeans(n_clusters=cluster_dict[sheet])
    clusters = kmeans.fit_predict(principal_components)

    # Visualizando em 3D
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    scatter = ax.scatter(principal_components[:, 0],
                         principal_components[:, 1],
                         principal_components[:, 2],
                         c=clusters, cmap='viridis')

    ax.set_title(f'Clusters para {sheet}')
    ax.set_xlabel('Componente Principal 1')
    ax.set_ylabel('Componente Principal 2')
    ax.set_zlabel('Componente Principal 3')
    plt.colorbar(scatter)
    plt.show()

"""# Coeficiente Silhueta - T1 3D"""

from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import pandas as pd
import os

# Define o caminho para o arquivo
file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'

# Lista de posições (abas) a serem lidas
positions = ["Atacantes", "Pontas", "Meio Ofensivos", "Volantes", "Laterais", "Zagueiros", "Goleiros"]

# Número ideal de clusters para cada posição
clusters = {
    "Atacantes": 3,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 3,
    "Laterais": 3,
    "Zagueiros": 3,
    "Goleiros": 3
}

# Carregar os dados
xls = pd.ExcelFile(file_path)

# Iterar por cada aba e calcular o coeficiente de silhueta
for position in positions:
    data = xls.parse(position)
    data = data.iloc[:, 10:]  # Selecionando colunas a partir de K

    # Padronização
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # PCA para redução de dimensionalidade para 3 componentes
    pca = PCA(n_components=3)
    pca_result = pca.fit_transform(data_scaled)

    # KMeans com o número ideal de clusters para a posição
    kmeans = KMeans(n_clusters=clusters[position], random_state=42).fit(pca_result)

    # Calcular coeficiente de silhueta
    silhouette_avg = silhouette_score(pca_result, kmeans.labels_)
    print(f"Para {position} com n_clusters = {clusters[position]}, o coeficiente médio de silhueta é: {silhouette_avg:.2f}")

"""# K Means - T2 3D"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Dicionário para o número ideal de clusters por posição
clusters_dict = {
    "Atacantes": 2,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 2,
    "Laterais": 2,
    "Zagueiros": 2,
    "Goleiros": 2
}

# Carregar o arquivo Excel
file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'

# Defina uma lista de cores. Aumente esta lista se você espera ter mais clusters.
colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'pink', 'cyan', 'magenta']

for sheet_name, n_clusters in clusters_dict.items():
    df = pd.read_excel(file_path, sheet_name=sheet_name)

    # Separar colunas numéricas
    data = df.select_dtypes(include=[np.number])

    # Transformação logarítmica para reduzir a dispersão
    data = np.log1p(data)

    # Padronizar os dados
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # Redução de dimensionalidade para 3 componentes
    pca = PCA(n_components=3)
    principal_components = pca.fit_transform(data_scaled)

    # Aplicação do KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(principal_components)
    df['Cluster'] = kmeans.labels_

    # Visualização 3D
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    for cluster_num in range(n_clusters):
        mask = df['Cluster'] == cluster_num
        ax.scatter(principal_components[mask, 0],
                   principal_components[mask, 1],
                   principal_components[mask, 2],
                   s=60, label=f'Cluster {cluster_num+1}', c=colors[cluster_num])

        # Plotando o centróide na cor preta
        centroid = kmeans.cluster_centers_[cluster_num]
        ax.scatter(centroid[0], centroid[1], centroid[2], s=120, c='black', marker='x')

        # Identificar linha de dados mais próxima do centróide
        closest, _ = min(enumerate(principal_components), key=lambda x: np.linalg.norm(x[1]-centroid))
        print(f"Centróide {cluster_num+1} da aba {sheet_name} está mais próximo da linha {closest+2} do Excel e é representado pela cor {colors[cluster_num]}.")

    ax.set_title(sheet_name)
    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_zlabel('PC3')
    ax.legend()
    plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

clusters_dict = {
    "Atacantes": 2,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 2,
    "Laterais": 2,
    "Zagueiros": 2,
    "Goleiros": 3
}

file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'
output_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Kmean3d .xlsx'

for sheet_name, n_clusters in clusters_dict.items():
    df = pd.read_excel(file_path, sheet_name=sheet_name)

    # Separar colunas numéricas
    data = df.select_dtypes(include=[np.number])

    # Transformação logarítmica para reduzir a dispersão
    data = np.log1p(data)

    # Padronizar os dados
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # Redução de dimensionalidade para 3 componentes
    pca = PCA(n_components=3)
    principal_components = pca.fit_transform(data_scaled)

    # Aplicação do KMeans com n_init definido explicitamente
    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
    kmeans.fit(principal_components)

    # Adicionar os rótulos do cluster ao DataFrame original
    df['Cluster'] = kmeans.labels_

    # Salvar a aba modificada no novo arquivo Excel
    df.to_excel(output_path, sheet_name=sheet_name, index=False)

"""# Coeficiente Silhueta - T2 3D"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Dicionário para o número ideal de clusters por posição
clusters_dict = {
    "Atacantes": 2,
    "Pontas": 2,
    "Meio Ofensivos": 2,
    "Volantes": 2,
    "Laterais": 2,
    "Zagueiros": 2,
    "Goleiros": 2
}

# Carregar o arquivo Excel
file_path = '/content/drive/MyDrive/Futebol/Dados/DB Fut 1.2 - Quintil - Posicoes.xlsx'

for sheet_name, n_clusters in clusters_dict.items():
    df = pd.read_excel(file_path, sheet_name=sheet_name)

    # Separar colunas numéricas
    data = df.select_dtypes(include=[np.number])

    # Transformação logarítmica para reduzir a dispersão
    data = np.log1p(data)

    # Padronizar os dados
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # Redução de dimensionalidade para 3 componentes
    pca = PCA(n_components=3)
    principal_components = pca.fit_transform(data_scaled)

    # Aplicação do KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(principal_components)

    # Calculando o coeficiente de silhueta
    score = silhouette_score(principal_components, clusters)
    print(f"Para {sheet_name} com n_clusters = {n_clusters}, o coeficiente médio de silhueta é: {score:.2f}")

"""# Heatmap - 1.0"""